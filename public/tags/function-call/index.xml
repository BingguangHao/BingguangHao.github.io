<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Function Call on Agent</title><link>https://bingguanghao.github.io/Agent/tags/function-call/</link><description>Recent content in Function Call on Agent</description><generator>Hugo</generator><language>en</language><lastBuildDate>Fri, 30 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://bingguanghao.github.io/Agent/tags/function-call/index.xml" rel="self" type="application/rss+xml"/><item><title>FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement</title><link>https://bingguanghao.github.io/Agent/post/chapter-1/</link><pubDate>Fri, 30 May 2025 00:00:00 +0000</pubDate><guid>https://bingguanghao.github.io/Agent/post/chapter-1/</guid><description>&lt;p>&lt;a href="https://github.com/BingguangHao/FunReason/">ðŸ“–GitHub&lt;/a> &lt;a href="https://arxiv.org/pdf/2505.20192">ðŸ“‘Paper&lt;/a> &lt;a href="https://huggingface.co/Bingguang/FunReason">ðŸ¤—Hugging Face&lt;/a>&lt;/p>
&lt;hr>
&lt;p>Function call becomes a fundamental concept in Large Language Models, enabling more efficient interaction.&lt;/p>
&lt;p>We propose FunReason, FunReason enhances LLMs&amp;rsquo; function calling by balancing reasoning and execution accuracy. Using automated data refinement and Self-Refinement Multiscale Loss (SRML), it generates high-quality training data for query parseability, reasoning coherence, and precise function calls. SRML optimizes both aspects during training, matching GPT-4o performance while minimizing forgetting. &lt;strong>We will release all the data that refined in this process, whcih contains 60k high quality CoT data for function call.&lt;/strong>&lt;/p></description></item></channel></rss>